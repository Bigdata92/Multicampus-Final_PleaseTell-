{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KoGPT2_Finetunning_06(브런치_여행_일반).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNKgG8uwXdSMT2MjcR25NQK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMRxBL-36iUd","executionInfo":{"status":"ok","timestamp":1637941647497,"user_tz":-540,"elapsed":481,"user":{"displayName":"JeongBeom Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719327316927255470"}},"outputId":"68b735a0-0775-4c5f-f01e-a301a7402334"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"ecZBqye06qLF"},"source":["!pip install transformers\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeOZ1ZU27Bu2"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M76ebr4U6til"},"source":["from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import GPT2LMHeadModel\n","from transformers import Trainer, TrainingArguments\n","from transformers import PreTrainedTokenizerFast\n","\n","def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset\n","\n","def load_data_collator(tokenizer, mlm = False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer = tokenizer,\n","        mlm = mlm,\n","    )\n","    return data_collator\n","\n","def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n","          per_device_train_batch_size, num_train_epochs, save_steps):\n","    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n","                bos_token = '</s>', eos_token = '</s>', unk_token = '<unk>',\n","                pad_token = '<pad>', mask_token = '<mask>')\n","    train_dataset = load_dataset(train_file_path, tokenizer)\n","    data_collator = load_data_collator(tokenizer)\n","\n","    tokenizer.save_pretrained(output_dir, legacy_format = False)\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","    model.save_pretrained(output_dir)\n","\n","    training_args = TrainingArguments(\n","        output_dir = output_dir,\n","        overwrite_output_dir = overwrite_output_dir,\n","        per_device_eval_batch_size = per_device_train_batch_size,\n","        num_train_epochs = num_train_epochs,\n","    )\n","\n","    trainer = Trainer(\n","        model = model,\n","        args = training_args,\n","        data_collator = data_collator,\n","        train_dataset = train_dataset,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model()\n","\n","train_file_path = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_일반)/Data/df_trip_trip.txt'\n","model_name = 'skt/kogpt2-base-v2'\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_일반)/Model'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 50.0\n","save_steps = 500\n","\n","train(train_file_path = train_file_path,\n","      model_name = model_name,\n","      output_dir = output_dir,\n","      overwrite_output_dir = overwrite_output_dir,\n","      per_device_train_batch_size = per_device_train_batch_size,\n","      num_train_epochs = num_train_epochs,\n","      save_steps = save_steps\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cb2w7_Fe9Sb-"},"source":["from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","from tqdm.notebook import tqdm\n","\n","def load_model(model_path):\n","  model = GPT2LMHeadModel.from_pretrained(model_path)\n","  return model\n","\n","def load_tokenizer(tokenizer_path):\n","  tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n","  return tokenizer\n","\n","def generate_text(sequence, max_length):\n","  model_path = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_일반)/Model'\n","  model = load_model(model_path)\n","  tokenizer = load_tokenizer(model_path)\n","  ids = tokenizer.encode(f'{sequence},', return_tensors = 'pt')\n","  final_outputs = model.generate(\n","      ids,\n","      do_sample = True,\n","      max_length = max_length,\n","      pad_token_id = model.config.pad_token_id,\n","      tok_p = 5,\n","      top_p = 0.90,\n","      no_repeat_ngram_size=3,\n","      repetition_penalty = 2.0,\n","  )\n","  return tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n","  # return tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n_Y3zu09Y11"},"source":["# sequence = '고기 고기가 올라가 있는 피자 한 판'\n","# max_length = 64\n","# sentence_list = []\n","# # print('input : ' + sequence + ' ' + refer)\n","# # for i in tqdm(range(5)):\n","# sentence_list.append(generate_text(sequence, max_length))\n","# sentence = generate_text(sequence, max_length)\n","# ch_sentence = sentence.split(sequence + ', ')[1:]\n","# print(f'입력 값 : {sequence}')\n","# ch_sentence = ch_sentence[0].replace('\\n', ' ')\n","# ch_sentence = ch_sentence.replace('.', '. ')\n","# ch_sentence = ch_sentence.replace('\"', '')\n","# ch_sentence = ch_sentence.replace('<unk>', '')\n","# ch_sentence = ch_sentence.replace('?', '? ')\n","# ch_sentence = ch_sentence.replace('!', '! ')\n","# ch_sentence = ch_sentence.replace('  ', ' ')\n","# ch_sentence = ch_sentence.replace('  ', ' ')\n","# ch_sentence = ch_sentence.replace('  ', ' ')\n","# ch_sentence = ch_sentence.replace('다. ', '다.\\n')\n","# print(ch_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J22IsP2me3w9"},"source":["!pip install git+https://github.com/ssut/py-hanspell.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1TexFJPeoko"},"source":["from hanspell import spell_checker"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqlhZHMFdpwc"},"source":["def spell_check(sequence):\n","    result = spell_checker.check(sequence)\n","    checked_sequence = result.checked\n","    return checked_sequence\n","\n","def result_sequence(sequence, max_length):\n","    sequence1 = generate_text(sequence, max_length)\n","    sequence2 = sequence1.split(f'{sequence}, ')[1]\n","    sequence3 = spell_check(sequence2)\n","    sequence4 = sequence3.replace('  ', ' ')\n","    sequence5 =  sequence4.replace('. ', '.. ')\n","    sequence6 = ' '.join(sequence5.split('. ')[:-1])\n","    sequence7 = spell_check(sequence6)\n","    return sequence6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2l2o3Bzds_l"},"source":["from tqdm.notebook import tqdm\n","\n","sentence = '고기 고기가 올라가 있는 피자 한 판'\n","sequence_list = []\n","for _ in tqdm(range(5)):\n","    sequence = result_sequence(sentence, 64)\n","    sequence_list.append(sequence)\n","sequence = ' '.join(sequence_list)\n","sequence"],"execution_count":null,"outputs":[]}]}