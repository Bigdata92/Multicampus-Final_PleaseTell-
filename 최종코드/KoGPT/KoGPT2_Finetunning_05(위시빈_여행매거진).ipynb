{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KoGPT2_Finetunning_05(위시빈_여행매거진).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPaPmZ+fdIVdrvCBjJaG+ok"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMRxBL-36iUd","executionInfo":{"status":"ok","timestamp":1637915270600,"user_tz":-540,"elapsed":30214,"user":{"displayName":"JeongBeom Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719327316927255470"}},"outputId":"ebc109ef-03dd-4ee6-cebe-7bf8ccaf839e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"ecZBqye06qLF"},"source":["!pip install transformers\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeOZ1ZU27Bu2"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M76ebr4U6til"},"source":["from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import GPT2LMHeadModel\n","from transformers import Trainer, TrainingArguments\n","from transformers import PreTrainedTokenizerFast\n","\n","def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset\n","\n","def load_data_collator(tokenizer, mlm = False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer = tokenizer,\n","        mlm = mlm,\n","    )\n","    return data_collator\n","\n","def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n","          per_device_train_batch_size, num_train_epochs, save_steps):\n","    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n","                bos_token = '</s>', eos_token = '</s>', unk_token = '<unk>',\n","                pad_token = '<pad>', mask_token = '<mask>')\n","    train_dataset = load_dataset(train_file_path, tokenizer)\n","    data_collator = load_data_collator(tokenizer)\n","\n","    tokenizer.save_pretrained(output_dir, legacy_format = False)\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","    model.save_pretrained(output_dir)\n","\n","    training_args = TrainingArguments(\n","        output_dir = output_dir,\n","        overwrite_output_dir = overwrite_output_dir,\n","        per_device_eval_batch_size = per_device_train_batch_size,\n","        num_train_epochs = num_train_epochs,\n","    )\n","\n","    trainer = Trainer(\n","        model = model,\n","        args = training_args,\n","        data_collator = data_collator,\n","        train_dataset = train_dataset,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model()\n","\n","train_file_path = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/WishBeen(여행매거진)/Data/df_trip_wishbeen.txt'\n","model_name = 'skt/kogpt2-base-v2'\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/WishBeen(여행매거진)/Model'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 50.0\n","save_steps = 500\n","\n","train(train_file_path = train_file_path,\n","      model_name = model_name,\n","      output_dir = output_dir,\n","      overwrite_output_dir = overwrite_output_dir,\n","      per_device_train_batch_size = per_device_train_batch_size,\n","      num_train_epochs = num_train_epochs,\n","      save_steps = save_steps\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cb2w7_Fe9Sb-"},"source":["from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","from tqdm.notebook import tqdm\n","\n","def load_model(model_path):\n","  model = GPT2LMHeadModel.from_pretrained(model_path)\n","  return model\n","\n","def load_tokenizer(tokenizer_path):\n","  tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n","  return tokenizer\n","\n","def generate_text(sequence, max_lenth):\n","  model_path = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/WishBeen(여행매거진)/Model'\n","  model = load_model(model_path)\n","  tokenizer = load_tokenizer(model_path)\n","  ids = tokenizer.encode(f'{sequence},', return_tensors = 'pt')\n","  final_outputs = model.generate(\n","      ids,\n","      do_sample = True,\n","      max_length = max_length,\n","      pad_token_id = model.config.pad_token_id,\n","      tok_k = 5,\n","      top_p = 0.90,\n","      repetition_penalty = 2.0,\n","  )\n","  return tokenizer.decode(final_outputs[0])\n","  # return tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n","\n","sequence = '코끼리 앞에서 남자가 사진을 찍고있다.'\n","max_length = 128\n","sentence_list = []\n","# print('input : ' + sequence + ' ' + refer)\n","# for i in tqdm(range(5)):\n","sentence_list.append(generate_text(sequence, max_length))\n","sentence = generate_text(sequence, max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n_Y3zu09Y11","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637920762724,"user_tz":-540,"elapsed":12,"user":{"displayName":"JeongBeom Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719327316927255470"}},"outputId":"d55cd812-7e54-4181-cd1e-a03009bbf15c"},"source":["ch_sentence = sentence.split(sequence + ', ')[1:]\n","print(f'입력 값 : {sequence}')\n","ch_sentence = ch_sentence[0].replace('\\n', ' ')\n","ch_sentence = ch_sentence.replace('.', '. ')\n","ch_sentence = ch_sentence.replace('\"', '')\n","ch_sentence = ch_sentence.replace('<unk>', '')\n","ch_sentence = ch_sentence.replace('?', '? ')\n","ch_sentence = ch_sentence.replace('!', '! ')\n","ch_sentence = ch_sentence.replace('  ', ' ')\n","ch_sentence = ch_sentence.replace('  ', ' ')\n","ch_sentence = ch_sentence.replace('  ', ' ')\n","ch_sentence = ch_sentence.replace('다. ', '다.\\n')\n","print(ch_sentence)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["입력 값 : 코끼리 앞에서 남자가 사진을 찍고있다.\n","Miss India라는 노래. 원래 이 땅에는 수많은 소수민족이 모여 살고 있다.\n","마쭈와 윈난성 (Myanmarang) 두 개의 나라 수단은 한때 아프리카에서 가장 강성했던 곳이었다.\n","그런데 그 힘은 이리도 독보적이었던 것. 오랜 내전 끝에 2011년 말에 패전드가 됐고, 남수단이 독립하면서두만 남게 되었다.\n","그때 베이루트가 그토록 바라던 민족이 다시 뭉친 것이다.\n","여기는 난민과 혼자인 모습이 너무나 똑같기 때문이다.\n","배낭을 메는 여행자들이 넘나들썩한 옷깃의 끈을 붙잡고서라도 자신이 살던 곳을 그리려고 잔뜩\n"]}]}]}