{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KoGPT2_Finetunning_07(브런치_여행_통합).ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP/LsLnrg5dklsuGOzs5cbi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ba587f590d204ce9aed3428ef4eca02a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_74de657d063d4555afee221c905d6844","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1e0b03330ed74b338159b7a4cd405aa3","IPY_MODEL_7186f94f03f54de38792beab6d9a92c3","IPY_MODEL_3957fc5af6314550901ba494fa58df2b"]}},"74de657d063d4555afee221c905d6844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1e0b03330ed74b338159b7a4cd405aa3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f402c01c7044d948719bc796845a612","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d5c9e0cd7404830a4aac47a50770d72"}},"7186f94f03f54de38792beab6d9a92c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_54231cf7094e472388cea5bf493726ee","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3f127d1c4f194722be1b1505181918ac"}},"3957fc5af6314550901ba494fa58df2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1ef66a273ce9408d8c49dc526683983a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:11&lt;00:00,  5.45s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85e2ade64b664474a159843f93ec8c0c"}},"4f402c01c7044d948719bc796845a612":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d5c9e0cd7404830a4aac47a50770d72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"54231cf7094e472388cea5bf493726ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3f127d1c4f194722be1b1505181918ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ef66a273ce9408d8c49dc526683983a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85e2ade64b664474a159843f93ec8c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMRxBL-36iUd","executionInfo":{"status":"ok","timestamp":1637927970259,"user_tz":-540,"elapsed":24820,"user":{"displayName":"JeongBeom Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719327316927255470"}},"outputId":"c530bbac-6e36-4eed-bfc5-032d8ef499b8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"ecZBqye06qLF"},"source":["!pip install transformers\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeOZ1ZU27Bu2"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M76ebr4U6til"},"source":["from transformers import TextDataset, DataCollatorForLanguageModeling\n","from transformers import GPT2LMHeadModel\n","from transformers import Trainer, TrainingArguments\n","from transformers import PreTrainedTokenizerFast\n","\n","def load_dataset(file_path, tokenizer, block_size = 128):\n","    dataset = TextDataset(\n","        tokenizer = tokenizer,\n","        file_path = file_path,\n","        block_size = block_size,\n","    )\n","    return dataset\n","\n","def load_data_collator(tokenizer, mlm = False):\n","    data_collator = DataCollatorForLanguageModeling(\n","        tokenizer = tokenizer,\n","        mlm = mlm,\n","    )\n","    return data_collator\n","\n","def train(train_file_path, model_name, output_dir, overwrite_output_dir,\n","          per_device_train_batch_size, num_train_epochs, save_steps):\n","    tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name,\n","                bos_token = '</s>', eos_token = '</s>', unk_token = '<unk>',\n","                pad_token = '<pad>', mask_token = '<mask>')\n","    train_dataset = load_dataset(train_file_path, tokenizer)\n","    data_collator = load_data_collator(tokenizer)\n","\n","    tokenizer.save_pretrained(output_dir, legacy_format = False)\n","    model = GPT2LMHeadModel.from_pretrained(model_name)\n","    model.save_pretrained(output_dir)\n","\n","    training_args = TrainingArguments(\n","        output_dir = output_dir,\n","        overwrite_output_dir = overwrite_output_dir,\n","        per_device_eval_batch_size = per_device_train_batch_size,\n","        num_train_epochs = num_train_epochs,\n","    )\n","\n","    trainer = Trainer(\n","        model = model,\n","        args = training_args,\n","        data_collator = data_collator,\n","        train_dataset = train_dataset,\n","    )\n","\n","    trainer.train()\n","    trainer.save_model()\n","\n","train_file_path = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Data/df_trip.txt'\n","model_name = 'skt/kogpt2-base-v2'\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model'\n","overwrite_output_dir = False\n","per_device_train_batch_size = 8\n","num_train_epochs = 50.0\n","save_steps = 500\n","\n","train(train_file_path = train_file_path,\n","      model_name = model_name,\n","      output_dir = output_dir,\n","      overwrite_output_dir = overwrite_output_dir,\n","      per_device_train_batch_size = per_device_train_batch_size,\n","      num_train_epochs = num_train_epochs,\n","      save_steps = save_steps\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cb2w7_Fe9Sb-"},"source":["from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n","from tqdm.notebook import tqdm\n","\n","def load_model(model_path):\n","  model = GPT2LMHeadModel.from_pretrained(model_path)\n","  return model\n","\n","def load_tokenizer(tokenizer_path):\n","  tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n","  return tokenizer\n","\n","def generate_text(sequence, max_length):\n","  model_path = '/content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model'\n","  model = load_model(model_path)\n","  tokenizer = load_tokenizer(model_path)\n","  ids = tokenizer.encode(f'{sequence},', return_tensors = 'pt')\n","  final_outputs = model.generate(\n","      ids,\n","      do_sample = True,\n","      max_length = max_length,\n","      pad_token_id = model.config.pad_token_id,\n","      tok_k = 5,\n","      top_p = 0.90,\n","      no_repeat_ngram_size=3,\n","      repetition_penalty = 2.0,\n","  )\n","  return tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n","  # return tokenizer.decode(final_outputs[0], skip_special_tokens = True)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n_Y3zu09Y11"},"source":["# sequence = '고기 고기가 올라가 있는 피자 한 판'\n","# max_length = 64\n","# sentence_list = []\n","# # print('input : ' + sequence + ' ' + refer)\n","# # for i in tqdm(range(5)):\n","# sentence_list.append(generate_text(sequence, max_length))\n","# sentence = generate_text(sequence, max_length)\n","# ch_sentence = sentence.split(sequence + ', ')[1:]\n","# print(f'입력 값 : {sequence}')\n","# ch_sentence = ch_sentence[0].replace('\\n', ' ')\n","# ch_sentence = ch_sentence.replace('.', '. ')\n","# ch_sentence = ch_sentence.replace('\"', '')\n","# ch_sentence = ch_sentence.replace('<unk>', '')\n","# ch_sentence = ch_sentence.replace('?', '? ')\n","# ch_sentence = ch_sentence.replace('!', '! ')\n","# ch_sentence = ch_sentence.replace('  ', ' ')\n","# ch_sentence = ch_sentence.replace('  ', ' ')\n","# ch_sentence = ch_sentence.replace('  ', ' ')\n","# ch_sentence = ch_sentence.replace('다. ', '다.\\n')\n","# print(ch_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J22IsP2me3w9"},"source":["!pip install git+https://github.com/ssut/py-hanspell.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a1TexFJPeoko"},"source":["from hanspell import spell_checker"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mqlhZHMFdpwc"},"source":["def spell_check(sequence):\n","    result = spell_checker.check(sequence)\n","    checked_sequence = result.checked\n","    return checked_sequence\n","\n","def result_sequence(sequence, max_length):\n","    sequence1 = generate_text(sequence, max_length)\n","    sequence2 = sequence1.split(f'{sequence}, ')[1]\n","    sequence3 = spell_check(sequence2)\n","    sequence4 = sequence3.replace('  ', ' ')\n","    sequence5 =  sequence4.replace('. ', '.. ')\n","    sequence6 = ' '.join(sequence5.split('. ')[:-1])\n","    sequence7 = spell_check(sequence6)\n","    return sequence6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2l2o3Bzds_l","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ba587f590d204ce9aed3428ef4eca02a","74de657d063d4555afee221c905d6844","1e0b03330ed74b338159b7a4cd405aa3","7186f94f03f54de38792beab6d9a92c3","3957fc5af6314550901ba494fa58df2b","4f402c01c7044d948719bc796845a612","8d5c9e0cd7404830a4aac47a50770d72","54231cf7094e472388cea5bf493726ee","3f127d1c4f194722be1b1505181918ac","1ef66a273ce9408d8c49dc526683983a","85e2ade64b664474a159843f93ec8c0c"]},"executionInfo":{"status":"ok","timestamp":1637930023366,"user_tz":-540,"elapsed":11349,"user":{"displayName":"JeongBeom Cho","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"17719327316927255470"}},"outputId":"df1290ec-9104-4ba3-d01d-80d43b880775"},"source":["from tqdm.notebook import tqdm\n","\n","sentence = '고기 고기가 올라가 있는 피자 한 판'\n","sequence_list = []\n","for _ in tqdm(range(2)):\n","    sequence = result_sequence(sentence, 64)\n","    sequence_list.append(sequence)\n","sequence = ' '.join(sequence_list)\n","sequence"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba587f590d204ce9aed3428ef4eca02a","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/added_tokens.json. We won't load it.\n","loading file None\n","loading file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/special_tokens_map.json\n","loading file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/tokenizer_config.json\n","loading file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/tokenizer.json\n","loading configuration file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"skt/kogpt2-base-v2\",\n","  \"_num_labels\": 1,\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n","  \"bos_token_id\": 0,\n","  \"created_date\": \"2021-04-28\",\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0\n","  },\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"license\": \"CC-BY-NC-SA 4.0\",\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_inner\": null,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"pad_token_id\": 3,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 51200\n","}\n","\n","loading weights file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/pytorch_model.bin\n","All model checkpoint weights were used when initializing GPT2LMHeadModel.\n","\n","All the weights of GPT2LMHeadModel were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n","Didn't find file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/added_tokens.json. We won't load it.\n","loading file None\n","loading file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/special_tokens_map.json\n","loading file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/tokenizer_config.json\n","loading file /content/drive/MyDrive/Colab Notebooks/팀프로젝트/빅데이터 지능형 서비스과정(최종프로젝트)/KoGPT2_FineTunning/Branch(여행_통합)/Model/tokenizer.json\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'그리고 마지막으로 저녁을 먹을 식당은 어디쯤에 있는지 맥주 마시기 좋은 바는 또 어떤지 눈여겨 봐두고 숙소에서 가장 가까운 슈퍼와 인상 깊은 주인이 과일 주스를 갈아주는 노점도 익혀둔다. 그리고 다른 종류의 소스인 유부 가락국수. 고등어라고 하면 한국에서도 최근 들어 조금씩 대중화되고는 있지만 여전히 어른들이 가지고 노는 비싼 장난감 정도로 치부를 하고 있다. 가끔은 너무 비싸므로 사지 않는 것이 나을지도 모른다.'"]},"metadata":{},"execution_count":10}]}]}