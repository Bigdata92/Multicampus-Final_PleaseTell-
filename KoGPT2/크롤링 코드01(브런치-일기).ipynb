{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 데이터 크롤링\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "from tqdm.notebook import tqdm\n",
    "URL = 'https://brunch.co.kr'\n",
    "driver = webdriver.Chrome('C:\\\\Users\\\\Bestc\\\\Desktop\\\\빅데이터 지능형 서비스 개발과정(멀티캠퍼스)\\\\설치프로그램\\\\Selenium\\\\chromedriver.exe')\n",
    "driver.get(URL)\n",
    "time.sleep(2)\n",
    "driver.find_element_by_xpath('//*[@id=\"mArticle\"]/div[3]/div[1]/div/a[24]').click()\n",
    "time.sleep(2)\n",
    "driver.close()\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "driver.find_element_by_xpath('//*[@id=\"keywordItemListBlock\"]/a[11]').click()\n",
    "time.sleep(2)\n",
    "driver.close()\n",
    "driver.switch_to.window(driver.window_handles[0])\n",
    "driver.find_element_by_xpath('//*[@id=\"keywordItemListBlock\"]/a[9]').click()\n",
    "time.sleep(2)\n",
    "driver.close()\n",
    "driver.switch_to.window(driver.window_handles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(5000)):\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3df61ad128745169d0808dbf8a6f6a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1672 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-57bf84c94d9d>:6: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
      "<ipython-input-20-57bf84c94d9d>:9: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
      "<ipython-input-20-57bf84c94d9d>:10: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# title_list, contents_list = [], []\n",
    "for i in tqdm(range(1329, 3001)):\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    time.sleep(3)\n",
    "    raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
    "    raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n",
    "    # contents = re.sub('[\\n]', '', raw_contents)\n",
    "    # title = re.sub('[\\n]', '', raw_title)\n",
    "    title_list.append(raw_title)\n",
    "    contents_list.append(raw_contents)\n",
    "    time.sleep(3)\n",
    "    driver.close()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3001, 3001)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents_list), len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#송희의좋은생각\\n결국 마음에서시작\\n나의태도에서 시작\\n그리고 인생은 장거리다 천천히 가는방향 목적지만 생각하자\\n나는 내일 죽지않을것이다\\n오늘을 살지만 내일도 있는 사람\\n진짜 기독교와 예술을빼면?\\n오늘은 먹고자고만 하는날 쉴거다 어제넘우울\\n문제풀구\\n어제보다가 역시 하루가쌓여서 어제의내가 오늘의나를 만든다\\n아침요가조지고\\n유산균 먹고\\n뒹굴거리기 좋다\\n밥묵고\\n먹고간식\\n\\n칼로리채우기..\\n걍 오전에 더안먹고 쉬려규 지금 먹어버리기\\n\\n하 넘귀찮다...\\n오늘도 오전은 이걸로끝내야지..나눠먹기귀찬아 주말만 이렇게..\\n쓰고싶을때마누쓰는\\natopic 아토피\\n자란다도 올리구 ㅋㅋㅋ 당근찰구\\n심심해서 옷장절리두 하구 ㅋㅋ 이제 간다웅동\\n웜업 트레드밀 18경사 6분속도5\\n푸쉬업\\n덤벨 위로올리는거\\n스쿼트\\n스쿼트하면사 암워킹이랑\\n샤레레\\n암워킹\\n레그익스텐션\\n힙어브덕션\\n런지 존내많이\\n클로이팅복근\\n레베카복근\\n아침 요가\\n오늘 운동개장먹음\\n\\n집가서!\\n먹구\\n아것도 참막어보는데맛이땅\\n고구마말랭이 이거 맛있군\\n영어모임하구 동생이랑 책사러가기\\n영어모임 짱잼탱..8ㅅ8\\n오늘 하루 힐링이다..8ㅅ8\\n좋은 사람들과 공부는 늘 즐거워!\\n갖고싶으네 넘비쌈 ㅎ\\n걸어가다니까 죽어도 안감 버스탐 ㅋㅋ\\n와중에저거 사고싶다 이미샀눈데내서카일\\n후..피곤.....한것도없는것같은ㄷ 하루가 정신없이겄구만 ㅋㅋㅋㅋㅋ\\n책한권가고 간식 ㅎ이것도 마지막\\n먹고 고정간식 오늘은 이거로 그만먹을래 ㅎ\\n일기도 쓰고 ㅋㅋ 저거막있음\\n근데왜인바디 ㅏ체만 ㅠ\\n흑인바디 믿지밀자\\n근데 아까 책 겨환해주신다고(빵꾸나있었음)\\n랬는데귀찬휴\\n\\n씻고디비저 잘가 빠22완벽하게 핼복한날'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'제목' : title_list, '내용' : contents_list})\n",
    "df['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#송희의좋은생각\n",
      "결국 마음에서시작\n",
      "나의태도에서 시작\n",
      "그리고 인생은 장거리다 천천히 가는방향 목적지만 생각하자\n",
      "나는 내일 죽지않을것이다\n",
      "오늘을 살지만 내일도 있는 사람\n",
      "진짜 기독교와 예술을빼면?\n",
      "오늘은 먹고자고만 하는날 쉴거다 어제넘우울\n",
      "문제풀구\n",
      "어제보다가 역시 하루가쌓여서 어제의내가 오늘의나를 만든다\n",
      "아침요가조지고\n",
      "유산균 먹고\n",
      "뒹굴거리기 좋다\n",
      "밥묵고\n",
      "먹고간식\n",
      "\n",
      "칼로리채우기..\n",
      "걍 오전에 더안먹고 쉬려규 지금 먹어버리기\n",
      "\n",
      "하 넘귀찮다...\n",
      "오늘도 오전은 이걸로끝내야지..나눠먹기귀찬아 주말만 이렇게..\n",
      "쓰고싶을때마누쓰는\n",
      "atopic 아토피\n",
      "자란다도 올리구 ㅋㅋㅋ 당근찰구\n",
      "심심해서 옷장절리두 하구 ㅋㅋ 이제 간다웅동\n",
      "웜업 트레드밀 18경사 6분속도5\n",
      "푸쉬업\n",
      "덤벨 위로올리는거\n",
      "스쿼트\n",
      "스쿼트하면사 암워킹이랑\n",
      "샤레레\n",
      "암워킹\n",
      "레그익스텐션\n",
      "힙어브덕션\n",
      "런지 존내많이\n",
      "클로이팅복근\n",
      "레베카복근\n",
      "아침 요가\n",
      "오늘 운동개장먹음\n",
      "\n",
      "집가서!\n",
      "먹구\n",
      "아것도 참막어보는데맛이땅\n",
      "고구마말랭이 이거 맛있군\n",
      "영어모임하구 동생이랑 책사러가기\n",
      "영어모임 짱잼탱..8ㅅ8\n",
      "오늘 하루 힐링이다..8ㅅ8\n",
      "좋은 사람들과 공부는 늘 즐거워!\n",
      "갖고싶으네 넘비쌈 ㅎ\n",
      "걸어가다니까 죽어도 안감 버스탐 ㅋㅋ\n",
      "와중에저거 사고싶다 이미샀눈데내서카일\n",
      "후..피곤.....한것도없는것같은ㄷ 하루가 정신없이겄구만 ㅋㅋㅋㅋㅋ\n",
      "책한권가고 간식 ㅎ이것도 마지막\n",
      "먹고 고정간식 오늘은 이거로 그만먹을래 ㅎ\n",
      "일기도 쓰고 ㅋㅋ 저거막있음\n",
      "근데왜인바디 ㅏ체만 ㅠ\n",
      "흑인바디 믿지밀자\n",
      "근데 아까 책 겨환해주신다고(빵꾸나있었음)\n",
      "랬는데귀찬휴\n",
      "\n",
      "씻고디비저 잘가 빠22완벽하게 핼복한날\n"
     ]
    }
   ],
   "source": [
    "print(df['내용'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#송희의좋은생각 결국 마음에서시작 나의태도에서 시작 그리고 인생은 장거리다 천천히 가는방향 목적지만 생각하자 나는 내일 죽지않을것이다 오늘을 살지만 내일도 있는 사람 진짜 기독교와 예술을빼면? 오늘은 먹고자고만 하는날 쉴거다 어제넘우울 문제풀구 어제보다가 역시 하루가쌓여서 어제의내가 오늘의나를 만든다 아침요가조지고 유산균 먹고 뒹굴거리기 좋다 밥묵고 먹고간식 칼로리채우기.. 걍 오전에 더안먹고 쉬려규 지금 먹어버리기 하 넘귀찮다... 오늘도 오전은 이걸로끝내야지..나눠먹기귀찬아 주말만 이렇게.. 쓰고싶을때마누쓰는 atopic 아토피 자란다도 올리구 ㅋㅋㅋ 당근찰구 심심해서 옷장절리두 하구 ㅋㅋ 이제 간다웅동 웜업 트레드밀 18경사 6분속도5 푸쉬업 덤벨 위로올리는거 스쿼트 스쿼트하면사 암워킹이랑 샤레레 암워킹 레그익스텐션 힙어브덕션 런지 존내많이 클로이팅복근 레베카복근 아침 요가 오늘 운동개장먹음 집가서! 먹구 아것도 참막어보는데맛이땅 고구마말랭이 이거 맛있군 영어모임하구 동생이랑 책사러가기 영어모임 짱잼탱..8ㅅ8 오늘 하루 힐링이다..8ㅅ8 좋은 사람들과 공부는 늘 즐거워! 갖고싶으네 넘비쌈 ㅎ 걸어가다니까 죽어도 안감 버스탐 ㅋㅋ 와중에저거 사고싶다 이미샀눈데내서카일 후..피곤.....한것도없는것같은ㄷ 하루가 정신없이겄구만 ㅋㅋㅋㅋㅋ 책한권가고 간식 ㅎ이것도 마지막 먹고 고정간식 오늘은 이거로 그만먹을래 ㅎ 일기도 쓰고 ㅋㅋ 저거막있음 근데왜인바디 ㅏ체만 ㅠ 흑인바디 믿지밀자 근데 아까 책 겨환해주신다고(빵꾸나있었음) 랬는데귀찬휴 씻고디비저 잘가 빠22완벽하게 핼복한날'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content = df['내용'][0].replace('\\n', ' ')\n",
    "test_content.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Workspace/python/빅데이터 지능형서비스 개발 팀프로젝트/Final Project/Data/KoGPT2_Data/Crawling Data/'\n",
    "df.to_csv(path + 'Crawling(brunch_일기_1~3000).csv', index = False)\n",
    "df['내용'].to_csv(path + 'Crawling(brunch_일기_1~3000).txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#송희의좋은생각\\n결국 마음에서시작\\n나의태도에서 시작\\n그리고 인생은 장거리다 천천히 가는방향 목적지만 생각하자\\n나는 내일 죽지않을것이다\\n오늘을 살지만 내일도 있는 사람\\n진짜 기독교와 예술을빼면?\\n오늘은 먹고자고만 하는날 쉴거다 어제넘우울\\n문제풀구\\n어제보다가 역시 하루가쌓여서 어제의내가 오늘의나를 만든다\\n아침요가조지고\\n유산균 먹고\\n뒹굴거리기 좋다\\n밥묵고\\n먹고간식\\n\\n칼로리채우기..\\n걍 오전에 더안먹고 쉬려규 지금 먹어버리기\\n\\n하 넘귀찮다...\\n오늘도 오전은 이걸로끝내야지..나눠먹기귀찬아 주말만 이렇게..\\n쓰고싶을때마누쓰는\\natopic 아토피\\n자란다도 올리구 ㅋㅋㅋ 당근찰구\\n심심해서 옷장절리두 하구 ㅋㅋ 이제 간다웅동\\n웜업 트레드밀 18경사 6분속도5\\n푸쉬업\\n덤벨 위로올리는거\\n스쿼트\\n스쿼트하면사 암워킹이랑\\n샤레레\\n암워킹\\n레그익스텐션\\n힙어브덕션\\n런지 존내많이\\n클로이팅복근\\n레베카복근\\n아침 요가\\n오늘 운동개장먹음\\n\\n집가서!\\n먹구\\n아것도 참막어보는데맛이땅\\n고구마말랭이 이거 맛있군\\n영어모임하구 동생이랑 책사러가기\\n영어모임 짱잼탱..8ㅅ8\\n오늘 하루 힐링이다..8ㅅ8\\n좋은 사람들과 공부는 늘 즐거워!\\n갖고싶으네 넘비쌈 ㅎ\\n걸어가다니까 죽어도 안감 버스탐 ㅋㅋ\\n와중에저거 사고싶다 이미샀눈데내서카일\\n후..피곤.....한것도없는것같은ㄷ 하루가 정신없이겄구만 ㅋㅋㅋㅋㅋ\\n책한권가고 간식 ㅎ이것도 마지막\\n먹고 고정간식 오늘은 이거로 그만먹을래 ㅎ\\n일기도 쓰고 ㅋㅋ 저거막있음\\n근데왜인바디 ㅏ체만 ㅠ\\n흑인바디 믿지밀자\\n근데 아까 책 겨환해주신다고(빵꾸나있었음)\\n랬는데귀찬휴\\n\\n씻고디비저 잘가 빠22완벽하게 핼복한날'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(path + 'Crawling(brunch_일기_1~3000).csv')\n",
    "df1['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12c66f1bc464a43aa26ff1b2ce561c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1427 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-e8caafbe1bdf>:6: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
      "<ipython-input-29-e8caafbe1bdf>:9: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
      "<ipython-input-29-e8caafbe1bdf>:10: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# title_list, contents_list = [], []\n",
    "for i in tqdm(range(4574, 6001)):\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    time.sleep(3)\n",
    "    raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
    "    raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n",
    "    # contents = re.sub('[\\n]', '', raw_contents)\n",
    "    # title = re.sub('[\\n]', '', raw_title)\n",
    "    title_list.append(raw_title)\n",
    "    contents_list.append(raw_contents)\n",
    "    time.sleep(3)\n",
    "    driver.close()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents_list), len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루 종일 무슨 생각을 할 수가 없다. 이유를 알 수 없는 두통이 어젯밤부터 지속되고 있다. 두통약을 먹어도 좋아지지가 않는다. 머리가 아프니, 뭘 해도 하는 것 같지가 않다. 일을 하고 있는데, 내가 제대로 하고 있는 게 맞는지를 모르겠다. \\n\\n왜 이렇게 두통이 심할까? 어제 급하게 먹은 점심이 체한 것 같아 소화제도 먹어봤고, 피곤해서 오는 두통인가 싶어, 두통약도 먹어봤지만, 좋아지지 않는다. 동생은 냉방병일 수도 있다고 했다. 생각해 보니, 요즘 직장에서도 집에서도 에어컨을 많이 키는 것 같기는 하다. 정말 냉방병이면 어떻게 치료를 해야 하나...\\n\\n요즘 왜 이렇게 컨디션이 좋아지지가 않는 걸까? 조금 좋아지는가 싶다가, 또 안 좋아지고, 또 안 좋아지고... 여름을 타는 건가? \\n\\n차라리 특별히 어딘가 딱 아프면, 병원에 가서 치료를 받거나 할 텐데, 딱히 어디가 막 아픈 건 아닌데, 묘하게 기분 나쁘게 여기저기가 좋지 않다. 이런 건 뭐... 컨디션이 좋지 않다라고밖에 설명을 할 수가 없다. 그런데 컨디션이 좋지 않은 건 어떻게 치료를 해야 하는 걸까?\\n\\n요즘 좀 뜸하게 먹던 영양제도 다시 먹고, 비타민도 더 챙겨 먹고 하는데... 떨어진 컨디션이 다시 회복되는 것 같지 않다. \\n\\n이 상태에서 빨리 벗어나고 싶은데... 어떻게 해야 할지 정말 모르겠다. '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'제목' : title_list, '내용' : contents_list})\n",
    "df['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루 종일 무슨 생각을 할 수가 없다. 이유를 알 수 없는 두통이 어젯밤부터 지속되고 있다. 두통약을 먹어도 좋아지지가 않는다. 머리가 아프니, 뭘 해도 하는 것 같지가 않다. 일을 하고 있는데, 내가 제대로 하고 있는 게 맞는지를 모르겠다.  왜 이렇게 두통이 심할까? 어제 급하게 먹은 점심이 체한 것 같아 소화제도 먹어봤고, 피곤해서 오는 두통인가 싶어, 두통약도 먹어봤지만, 좋아지지 않는다. 동생은 냉방병일 수도 있다고 했다. 생각해 보니, 요즘 직장에서도 집에서도 에어컨을 많이 키는 것 같기는 하다. 정말 냉방병이면 어떻게 치료를 해야 하나... 요즘 왜 이렇게 컨디션이 좋아지지가 않는 걸까? 조금 좋아지는가 싶다가, 또 안 좋아지고, 또 안 좋아지고... 여름을 타는 건가?  차라리 특별히 어딘가 딱 아프면, 병원에 가서 치료를 받거나 할 텐데, 딱히 어디가 막 아픈 건 아닌데, 묘하게 기분 나쁘게 여기저기가 좋지 않다. 이런 건 뭐... 컨디션이 좋지 않다라고밖에 설명을 할 수가 없다. 그런데 컨디션이 좋지 않은 건 어떻게 치료를 해야 하는 걸까? 요즘 좀 뜸하게 먹던 영양제도 다시 먹고, 비타민도 더 챙겨 먹고 하는데... 떨어진 컨디션이 다시 회복되는 것 같지 않다.  이 상태에서 빨리 벗어나고 싶은데... 어떻게 해야 할지 정말 모르겠다. '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content = df['내용'][0].replace('\\n', ' ')\n",
    "test_content.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Workspace/python/빅데이터 지능형서비스 개발 팀프로젝트/Final Project/Data/KoGPT2_Data/Crawling Data/'\n",
    "df.to_csv(path + 'Crawling(brunch_일기_3001~6000).csv', index = False)\n",
    "df['내용'].to_csv(path + 'Crawling(brunch_일기_3001~6000).txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루 종일 무슨 생각을 할 수가 없다. 이유를 알 수 없는 두통이 어젯밤부터 지속되고 있다. 두통약을 먹어도 좋아지지가 않는다. 머리가 아프니, 뭘 해도 하는 것 같지가 않다. 일을 하고 있는데, 내가 제대로 하고 있는 게 맞는지를 모르겠다. \\n\\n왜 이렇게 두통이 심할까? 어제 급하게 먹은 점심이 체한 것 같아 소화제도 먹어봤고, 피곤해서 오는 두통인가 싶어, 두통약도 먹어봤지만, 좋아지지 않는다. 동생은 냉방병일 수도 있다고 했다. 생각해 보니, 요즘 직장에서도 집에서도 에어컨을 많이 키는 것 같기는 하다. 정말 냉방병이면 어떻게 치료를 해야 하나...\\n\\n요즘 왜 이렇게 컨디션이 좋아지지가 않는 걸까? 조금 좋아지는가 싶다가, 또 안 좋아지고, 또 안 좋아지고... 여름을 타는 건가? \\n\\n차라리 특별히 어딘가 딱 아프면, 병원에 가서 치료를 받거나 할 텐데, 딱히 어디가 막 아픈 건 아닌데, 묘하게 기분 나쁘게 여기저기가 좋지 않다. 이런 건 뭐... 컨디션이 좋지 않다라고밖에 설명을 할 수가 없다. 그런데 컨디션이 좋지 않은 건 어떻게 치료를 해야 하는 걸까?\\n\\n요즘 좀 뜸하게 먹던 영양제도 다시 먹고, 비타민도 더 챙겨 먹고 하는데... 떨어진 컨디션이 다시 회복되는 것 같지 않다. \\n\\n이 상태에서 빨리 벗어나고 싶은데... 어떻게 해야 할지 정말 모르겠다. '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(path + 'Crawling(brunch_일기_3001~6000).csv')\n",
    "df2['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022f3099fac440fd957bd86f02e03ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1314 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-74-06f474bba2e7>:6: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
      "<ipython-input-74-06f474bba2e7>:9: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
      "<ipython-input-74-06f474bba2e7>:10: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# title_list, contents_list = [], []\n",
    "for i in tqdm(range(7687, 9001)):\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    time.sleep(3)\n",
    "    raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
    "    raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n",
    "    # contents = re.sub('[\\n]', '', raw_contents)\n",
    "    # title = re.sub('[\\n]', '', raw_title)\n",
    "    title_list.append(raw_title)\n",
    "    contents_list.append(raw_contents)\n",
    "    time.sleep(3)\n",
    "    driver.close()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1685, 1685)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contents_list), len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'아는 사람은 아는 시집 <제주에 혼자 살고 술은 약해요>를 글 제목으로 패러디해보았다. 가족과 함께 살았던 시간, 혼자서 살았던 시간, 타인과 함께 살았던 시간 등을 거쳐 지금은 서울에서 둘이 살고 있다. 내 짝꿍과 고양이 셋. 술이라면 눈을 초롱초롱 반짝이며 일보 전진하던 나였지만 망할! 양진(피부염)에 걸린 이후로 술은 좋아만 하는 신세, 짝사랑하는 신세가 되고 말았다. \\n\\n혼자 살던 시절의 나는, 외롭고 불안정한 감정에 자주 함락당해 내가 사랑해 마지않는 술과 함께 떡처럼 뭉쳐 지내거나 다른 생각을 할 겨를 없이 일과 공부, 온갖 것에 대한 참견으로 일상에 빈틈을 허락하지 않음으로 내면의 공허한 눈깔(눈알 아니고 눈깔 맞음)과 마주치지 않기 위해 발버둥 치며 지냈다. 길지 않았던 결혼 생활을 지나며 결혼이란 제도와 내가 얼마나 안 맞는지 진저리 나도록 깨닫고 난 뒤, 생각보다 훨씬 더 큰 편견으로 나를 재단하는 사람들과 그로 인한 상처를 덤으로 얹고 결혼 밖의 세상으로 나올 수 있었다. \\n\\n잡지사, 출판사, 인터넷 서점, 영화포털 기자, 소셜커머스 운영자, 어린이 학원 강사, 1인 출판사 대표, 외주 프리랜서(기획, 취재), 홈쇼핑 모니터링 요원 등을 거쳐 현재는 웹툰 업계에 발을 담그고 있다. \\n\\n브런치에 둥지를 튼지는 오래지만 워낙 띄엄띄엄 글을 쓴 탓에 올 때마다 새롭다.(자랑인가;; 장점인가;; 모르겠고 그냥 팩트다) 우기 & 울기 매거진으로 발행하는 글에는 아무래도 주제가 그렇다 보니 정말 너무 우울해서 쓴 내가 봐도 한숨만 나오는 글이 가득해서 클릭하기도 겁이 나는 바람에 새로 매거진을 만들기로 했고, 첫 글을 올린다. \\n\\n아무래도, 매거진의 첫 글이니 자기소개를 이 정도는 해야 되지 않으려나 싶어서 아무도 시키지 않았으나 스스로 하고 싶어 쓴 소개글로 첫 글을 마무리한다.\\n\\n\\n아, 연휴도 다가오는데 오늘도 딱 한 잔 하고 싶은데.. 저렇게 맥주잔 속에서 표류하고 싶은데.. 지금 내 팔과 다리엔 양진의 처참한 흔적이 피딱지랑 진물로 고스란히 남아 있어서 오늘도 그냥 짝사랑만 하고 있다. \\n\\n매거진 이름을 뭘로 정할지 고민하다가 흑심 일기로 정했다. 브런치라 영어로 해야 하니 Black Heart Diary! \\n말 그대로 검은 마음(순백의 마음은 아니므로;)을 담은 일기이기도 하고, 소수이지만 소중한 시간을 내어 찾아와 글을 읽어주시는 분들의 마음에 들고 싶은 흑심을 담은 일기이기도 하고, (흑심) 연필로 쓴 일기이기도 하다는 그런 의미를 담아 정해보았다. 쓰고 보니 거창한 듯 하지만 사실 지금 이 글을 쓰고 있는 책상 앞에 어제 연필 샵(연남동에 있는 흑심)에서 사 온 연필 패키지를 보고 정한 것임을 고백한다. '"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'제목' : title_list, '내용' : contents_list})\n",
    "df['내용'][1565]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = df['내용'][0].replace('\\n', ' ')\n",
    "test_content.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Workspace/python/빅데이터 지능형서비스 개발 팀프로젝트/Final Project/Data/KoGPT2_Data/Crawling Data/'\n",
    "df.to_csv(path + 'Crawling(brunch_일기_6001~9000).csv', index = False)\n",
    "df['내용'].to_csv(path + 'Crawling(brunch_일기_6001~9000).txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(path + 'Crawling(brunch_일기_6001~9000).csv')\n",
    "df3['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "title_list, contents_list = [], []\n",
    "for i in tqdm(range(9001, 11120)):\n",
    "    driver.switch_to.window(driver.window_handles[0])\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_xpath(f'//*[@id=\"wrapArticle\"]/div[1]/ul/li[{i}]').click()\n",
    "    driver.switch_to.window(driver.window_handles[1])\n",
    "    time.sleep(3)\n",
    "    raw_contents = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[2]/div[1]').text\n",
    "    raw_title = driver.find_element_by_xpath('/html/body/div[3]/div[1]/div[1]/div/div[3]/h1').text\n",
    "    # contents = re.sub('[\\n]', '', raw_contents)\n",
    "    # title = re.sub('[\\n]', '', raw_title)\n",
    "    title_list.append(raw_title)\n",
    "    contents_list.append(raw_contents)\n",
    "    time.sleep(3)\n",
    "    driver.close()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contents_list), len(title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'제목' : title_list, '내용' : contents_list})\n",
    "df['내용'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = df['내용'][0].replace('\\n', ' ')\n",
    "test_content.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:/Workspace/python/빅데이터 지능형서비스 개발 팀프로젝트/Final Project/Data/KoGPT2_Data/Crawling Data/'\n",
    "df.to_csv(path + 'Crawling(brunch_일기_9001~11120).csv', index = False)\n",
    "df['내용'].to_csv(path + 'Crawling(brunch_일기_9001~11120).txt', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(path + 'Crawling(brunch_일기_9001~11120).csv')\n",
    "df4['내용'][0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe3bab233825460445dbfeea6dcd52ef5815a3fa13c858fea07ac830aa383284"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
